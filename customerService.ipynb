{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# import data and process it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the dataset containing questions and answers from a CSV file\n",
    "\n",
    "data = pd.read_csv('QAs.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display the first 30 rows of the dataset for an initial check\n",
    "\n",
    "data.head(30)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Create a list of (question, answer) pairs\n",
    "faq_pairs = list(zip(data['Question'], data['Answer']))\n",
    "\n",
    "# Print the first few question-answer pairs to verify correctness\n",
    "print(faq_pairs[:5])\n",
    "\n",
    "# Determine the number of unique questions in the dataset\n",
    "\n",
    "unique_questions = data['Question'].unique()\n",
    "NUM_CLASSES = len(unique_questions)\n",
    "\n",
    "print(f\"Number of unique questions: {NUM_CLASSES}\")\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters, numbers, etc., and retain only letters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase and strip spcaces\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "# Apply the preprocessing function to clean the 'Question' and 'Answer' columns\n",
    "data['cleaned_question'] = data['Question'].apply(preprocess_text)\n",
    "data['cleaned_answer'] = data['Answer'].apply(preprocess_text)\n",
    "\n",
    "# Check the first few rows to ensure the text was cleaned properly\n",
    "print(data[['cleaned_question', 'cleaned_answer']].head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic regression with TF-IDF"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Extract questions and answers as input (X) and output (y)\n",
    "questions = data['cleaned_question'].values\n",
    "answers = data['cleaned_answer'].values\n",
    "\n",
    "# Convert questions to numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(questions)\n",
    "y = answers\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\", classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained Logistic Regression model to a file\n",
    "with open('log_reg_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(log_reg, model_file)\n",
    "\n",
    "# Save the trained TF-IDF vectorizer to a file\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as vectorizer_file:\n",
    "    pickle.dump(vectorizer, vectorizer_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to predict an answer for a new question\n",
    "def predict_answer_log(new_question):\n",
    "    # Load the saved Logistic Regression model\n",
    "    with open('log_reg_model.pkl', 'rb') as model_file:\n",
    "        loaded_model = pickle.load(model_file)\n",
    "\n",
    "    # Load the saved TF-IDF vectorizer\n",
    "    with open('tfidf_vectorizer.pkl', 'rb') as vectorizer_file:\n",
    "        loaded_vectorizer = pickle.load(vectorizer_file)\n",
    "\n",
    "    # Convert the new question into TF-IDF features\n",
    "    new_question_tfidf = loaded_vectorizer.transform([new_question])\n",
    "\n",
    "    # Predict the answer using the loaded model\n",
    "    predicted_answer = loaded_model.predict(new_question_tfidf)\n",
    "\n",
    "    return predicted_answer[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_question = \"How can I detect vulnerabilities in my assets?\"\n",
    "predicted_answer = predict_answer_log(new_question)\n",
    "print(f\"Predicted Answer: {predicted_answer}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "print(\"Accuracy (Random Forest):\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Classification Report (Random Forest):\", classification_report(y_test, y_pred_rf))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the trained Random Forest model to a file\n",
    "with open('rand_for_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(log_reg, model_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to predict an answer for a new question\n",
    "def predict_answer_rand(new_question):\n",
    "    # Load the saved Random Forest model\n",
    "    with open('rand_for_model.pkl', 'rb') as model_file:\n",
    "        loaded_model = pickle.load(model_file)\n",
    "\n",
    "    # Load the saved TF-IDF vectorizer\n",
    "    with open('tfidf_vectorizer.pkl', 'rb') as vectorizer_file:\n",
    "        loaded_vectorizer = pickle.load(vectorizer_file)\n",
    "\n",
    "    # Convert the new question into TF-IDF features\n",
    "    new_question_tfidf = loaded_vectorizer.transform([new_question])\n",
    "\n",
    "    # Predict the answer using the loaded model\n",
    "    predicted_answer = loaded_model.predict(new_question_tfidf)\n",
    "\n",
    "    return predicted_answer[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_question = \"How can I detect vulnerabilities in my assets?\"\n",
    "predicted_answer = predict_answer_rand(new_question)\n",
    "print(f\"Predicted Answer: {predicted_answer}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pretrained BERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the 'cleaned_question' data\n",
    "tokens = tokenizer.batch_encode_plus(\n",
    "    data['cleaned_question'].tolist(),  # Tokenize the cleaned questions\n",
    "    max_length=128,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "# Convert the tokens into tensors\n",
    "input_ids = tokens['input_ids']  # Token IDs for each question\n",
    "attention_masks = tokens['attention_mask']  # Attention masks for each question\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "# Dummy labels (You need to replace this with actual labels from your dataset)\n",
    "labels = torch.tensor([0] * len(input_ids))  # Replace with actual labels\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(input_ids, labels, test_size=0.2)\n",
    "train_masks, test_masks = train_test_split(attention_masks, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=64)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "# Load BERT with a classification head (output layer)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=NUM_CLASSES)  # Set NUM_CLASSES to the number of classes in your dataset\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up training loop\n",
    "epochs = 4\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch_input_ids, batch_input_mask, batch_labels = batch\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_input_mask, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "for batch in test_dataloader:\n",
    "    batch_input_ids, batch_input_mask, batch_labels = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch_input_ids, attention_mask=batch_input_mask)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    eval_accuracy += (predictions == batch_labels).sum().item()\n",
    "\n",
    "eval_accuracy /= len(test_labels)\n",
    "print(f'Test Accuracy: {eval_accuracy:.2f}')\n",
    "print(f\"Loss: {total_loss / len(train_dataloader)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained('models/bert')\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained('models/bert')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('models/bert')  # Use the path where you saved your model\n",
    "tokenizer = BertTokenizer.from_pretrained('models/bert')\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "def find_best_answer(question, faq_pairs):\n",
    "    question_cleaned = preprocess_text(question)  # Assuming you have the same preprocess_text function\n",
    "\n",
    "    best_score = -float('inf')\n",
    "    best_answer = None\n",
    "\n",
    "    for (faq_question, faq_answer) in faq_pairs:\n",
    "        # Preprocess and tokenize the pair (question, possible answer)\n",
    "        inputs = tokenizer.encode_plus(\n",
    "            question_cleaned + \" [SEP] \" + preprocess_text(faq_question),\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Get input_ids and attention_mask tensors\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "\n",
    "        # Make prediction (logit scores)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Take the maximum logit as the score\n",
    "        score = torch.max(logits).item()\n",
    "\n",
    "        # Update the best answer if this one has a higher score\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_answer = faq_answer\n",
    "\n",
    "    return best_answer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question = \"How do I track application licenses?\"\n",
    "best_answer = find_best_answer(question, faq_pairs)\n",
    "\n",
    "print(f\"Best Answer: {best_answer}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pretrained SBERT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses,util\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Split data into training and validation sets (80% train, 20% validation)\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert training data into InputExample format\n",
    "train_examples = [InputExample(texts=[row['Question'], row['Answer']], label=1.0) for _, row in train_data.iterrows()]\n",
    "\n",
    "# Convert validation data into InputExample format\n",
    "validation_examples = [InputExample(texts=[row['Question'], row['Answer']], label=1.0) for _, row in val_data.iterrows()]\n",
    "\n",
    "# Create DataLoader for the training and validation data\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "\n",
    "# Define the loss function for similarity learning\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, validation_data, threshold=0.5):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for example in validation_data:\n",
    "            embeddings = model.encode([example.texts[0], example.texts[1]])\n",
    "            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "            predictions.append(similarity)\n",
    "            labels.append(example.label)\n",
    "\n",
    "    # Adjust threshold and classify based on similarity\n",
    "    predicted_labels = [1 if sim > threshold else 0 for sim in predictions]\n",
    "    accuracy = np.mean([1 if pred == true else 0 for pred, true in zip(predicted_labels, labels)])\n",
    "\n",
    "    # Return detailed metrics\n",
    "    return accuracy, predictions, predicted_labels, labels\n",
    "\n",
    "# Fine-tune the model and log metrics\n",
    "for epoch in range(3):\n",
    "    print(f'Epoch : {epoch+1}')\n",
    "\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=1,  # Train for one epoch at a time\n",
    "        warmup_steps=100\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model on validation data and extract metrics\n",
    "\n",
    "accuracy, predictions, predicted_labels, labels = evaluate_model(model, validation_examples, threshold=0.7)\n",
    "\n",
    "accuracy = accuracy_score(labels, predicted_labels)\n",
    "recall = recall_score(labels, predicted_labels)\n",
    "f1 = f1_score(labels, predicted_labels)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'recall: {recall}')\n",
    "print(f'f1 : {f1}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save('sbert_finetuned_model')\n",
    "\n",
    "# Load the fine-tuned model when needed\n",
    "model = SentenceTransformer('sbert_finetuned_model')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "import torch\n",
    "\n",
    "# Prepare FAQ answers as embeddings\n",
    "answers = [preprocess_text(answer) for _, answer in data['Answer'].items()]\n",
    "answer_embeddings = model.encode(answers, convert_to_tensor=True)\n",
    "\n",
    "# Define a function to find the best answer using the fine-tuned SBERT model\n",
    "def find_best_answer_sbert(question):\n",
    "    # Preprocess and encode the input question\n",
    "    question_cleaned = preprocess_text(question)\n",
    "    question_embedding = model.encode(question_cleaned, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarities between the question and FAQ answers\n",
    "    cosine_scores = util.pytorch_cos_sim(question_embedding, answer_embeddings)\n",
    "\n",
    "    # Find the answer with the highest similarity score\n",
    "    best_score_idx = torch.argmax(cosine_scores).item()\n",
    "    best_answer = answers[best_score_idx]  # Get the corresponding answer\n",
    "\n",
    "    return best_answer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "question = \"How can I detect vulnerabilities in my assets?\"\n",
    "best_answer = find_best_answer_sbert(question)\n",
    "print(f\"Best Answer: {best_answer}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
